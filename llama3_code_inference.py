# -*- coding: utf-8 -*-
"""LLAMA3_code_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QIr4sO2Qm0kI9bk26BWuYvdoC8GPBr81

## Installing and Importing Necessary Libraries and Dependencies
"""

# Installation for GPU llama-cpp-python
# uncomment and run the following code in case GPU is being used
!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q

# Installation for CPU llama-cpp-python
# uncomment and run the following code in case GPU is not being used
# !CMAKE_ARGS="-DLLAMA_CUBLAS=off" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q

"""**Note**:
- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.
- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***.
"""

# For installing the libraries & downloading models from HF Hub
!pip install huggingface_hub==0.35.3 pandas==2.2.2 tiktoken==0.12.0 pymupdf==1.26.5 langchain==0.3.27 langchain-community==0.3.31 chromadb==1.1.1 sentence-transformers==5.1.1 numpy==2.3.3 -q

"""**Note**:
- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.
- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***.
"""

#Libraries for processing dataframes,text
import json,os
import tiktoken
import pandas as pd

#Libraries for Loading Data, Chunking, Embedding, and Vector Databases
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma

#Libraries for downloading and loading the llm
from huggingface_hub import hf_hub_download
from llama_cpp import Llama

"""## Question Answering using LLM

#### Downloading and Loading the model
"""

import os, sys, io
from huggingface_hub import hf_hub_download
from llama_cpp import Llama

# ---- Jupyter/Colab fileno patch ----
class _FDProxy:
    def __init__(self, stream, fd):
        self._stream = stream
        self._fd = fd
    def write(self, data): return self._stream.write(data)
    def flush(self): return self._stream.flush()
    def fileno(self): return self._fd
    def isatty(self): return False
    def __getattr__(self, name): return getattr(self._stream, name)

if not hasattr(sys.stdout, "fileno"):
    sys.stdout = _FDProxy(sys.stdout, 1)
else:
    try:
        _ = sys.stdout.fileno()
    except io.UnsupportedOperation:
        sys.stdout = _FDProxy(sys.stdout, 1)

if not hasattr(sys.stderr, "fileno"):
    sys.stderr = _FDProxy(sys.stderr, 2)
else:
    try:
        _ = sys.stderr.fileno()
    except io.UnsupportedOperation:
        sys.stderr = _FDProxy(sys.stderr, 2)

# ---- Download model ----
repo_id = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
filename = "mistral-7b-instruct-v0.2.Q4_K_M.gguf"
model_path = hf_hub_download(repo_id=repo_id, filename=filename)

# ---- Load model ----
llm = Llama(
    model_path=model_path,
    n_ctx=4096,
    n_threads=max(2, os.cpu_count() // 2),
    n_gpu_layers=35,   # set 0 if CPU-only
    verbose=False
)

print("LLM loaded:", model_path)

"""#### Response"""

def response(query,max_tokens=128,temperature=0,top_p=0.95,top_k=50):
    model_output = llm(
      prompt=query,
      max_tokens=max_tokens,
      temperature=temperature,
      top_p=top_p,
      top_k=top_k
    )

    return model_output['choices'][0]['text']

"""### Query 1: What is the protocol for managing sepsis in a critical care unit?"""

q = "What is the protocol for managing sepsis in a critical care unit?"
base_answer = response(q, max_tokens=256, temperature=0.2, top_p=0.9, top_k=50)
print(base_answer)

import json
import pandas as pd
import re

with open("ontology_ready_metadata.json", "r") as f:
    metadata = json.load(f)

def extract_json(text):
    match = re.search(r"\{.*\}", text, re.DOTALL)
    if not match:
        raise ValueError("Model did not return JSON")
    return json.loads(match.group(0))

def prompt_for_table(table):
    cols = "\n".join(
        f"- {c['name']} ({c['data_type']})"
        for c in table.get("columns", [])
    )

    return f"""
Return ONLY valid JSON.

Table name: {table.get("table_name")}
Physical name: {table.get("physical_name")}
Domain: {table.get("domain")}
Table type: {table.get("table_type")}
Description: {table.get("description")}

Columns:
{cols}

Tasks:
1. Write one sentence explaining the table purpose.
2. Write one sentence explaining the table context (how it’s used and what one row represents).
3. For each column, write a plain-English definition understandable by non-technical users.

JSON format:
{{
  "table_name": "...",
  "table_purpose": "...",
  "table_context": "...",
  "columns": [
    {{
      "name": "...",
      "plain_english_definition": "..."
    }}
  ]
}}
"""

results = []

for table in metadata["tables"]:
    output = response(
        prompt_for_table(table),
        max_tokens=600,
        temperature=0.1
    )
    results.append(extract_json(output))

with open("generated_definitions.json", "w") as f:
    json.dump(results, f, indent=2)

print("DONE — generated definitions for", len(results), "tables")

import json
import pandas as pd

with open("generated_definitions.json", "r") as f:
    data = json.load(f)

rows = []
for table in data:
    for col in table["columns"]:
        rows.append({
            "table_name": table["table_name"],
            "table_purpose": table["table_purpose"],
            "table_context": table["table_context"],
            "column_name": col["name"],
            "plain_english_definition": col["plain_english_definition"]
        })

df = pd.DataFrame(rows)
df

df.to_csv("generated_definitions.csv", index=False)